{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob \n",
    "from typing import Tuple, Callable, Dict\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "from torch.nn import Module\n",
    "from PIL import Image \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'processed-celeba-small/processed_celeba_small/celeba'\n",
    "img_channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING THE PREPROCESSED DATASET, THE IMAGES ARE ALREADY CROPPED TO 64x64x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(size: Tuple[int, int]) -> Callable:\n",
    "    transforms = [ToTensor(), Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])]\n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetDirectory(Dataset):\n",
    "    def __init__(self, directory: str, transforms: Callable = None, extension: str = '.jpg'):\n",
    "        self.directory = directory\n",
    "        self.extension = extension\n",
    "        self.transforms = transforms if transforms is not None else get_transforms()\n",
    "        self.dataset = ImageFolder(root=directory, transform=self.transforms)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        return self.dataset[index][0]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to check dataset outputs\n",
    "# def check_dataset_outputs(dataset: Dataset):\n",
    "#     assert len(dataset) == 32600, 'The dataset should contain 32,600 images.'\n",
    "#     index = np.random.randint(len(dataset))\n",
    "#     image = dataset[index]\n",
    "#     assert image.shape == torch.Size([3, 64, 64]), 'You must reshape the images to be 64x64'\n",
    "#     assert image.min() >= -1 and image.max() <= 1, 'The images should range between -1 and 1.'\n",
    "#     print('Congrats, your dataset implementation passed all the tests')\n",
    "\n",
    "# Create the dataset\n",
    "dataset = DatasetDirectory(data_dir, get_transforms((64, 64)))\n",
    "\n",
    "\n",
    "print(f\"Actual number of images in the dataset: {len(dataset)}\")\n",
    "print(f\"Contents of the dataset directory '{data_dir}':\")\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(images):\n",
    "    return ((images +1.)/2.*255).astype(np.uint8)\n",
    "\n",
    "fig = plt.figure(figsize=(20,4))\n",
    "plot_size = 20\n",
    "for idx in np.arange(plot_size):\n",
    "    ax = fig.add_subplot(2, int(plot_size/2), idx+1, xticks=[], yticks=[])\n",
    "    img = dataset[idx].numpy()\n",
    "    img = np.transpose(img,(1,2,0))\n",
    "    img = denormalize(img)\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_channels = img_channels\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels=img_channels,out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        ])\n",
    "        self.final_layer = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=0)\n",
    "        \n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.final_layer(x)\n",
    "        x = x.view(-1,1,1,1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_discriminator(discriminator: torch.nn.Module):\n",
    "#     images = torch.randn(1, 3, 64, 64)\n",
    "#     score = discriminator(images)\n",
    "#     assert score.shape == torch.Size([1, 1, 1, 1]), 'The discriminator output should be a single score.'\n",
    "#     print('Congrats, your discriminator implementation passed all the tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "# check_discriminator(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_channels = img_channels\n",
    "        self.init_layers()\n",
    "\n",
    "    def init_layers(self):\n",
    "        self.inital = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.latent_dim, out_channels=512, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(True)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(True)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(True)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels=64, out_channels=img_channels, kernel_size=4, stride=2, padding=1),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        x = self.inital(x)\n",
    "        for blocks in self.blocks:\n",
    "            x = blocks(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_generator(generator: torch.nn.Module, latent_dim: int):\n",
    "#     latent_vector = torch.randn(1, latent_dim, 1, 1)\n",
    "#     image = generator(latent_vector)\n",
    "#     assert image.shape == torch.Size([1, 3, 64, 64]), 'The generator should output a 64x64x3 images.'\n",
    "#     print('Congrats, your generator implementation passed all the tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "generator = Generator(latent_dim)\n",
    "# check_generator(generator, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizers(generator: Module, discriminator:Module, lr=0.0001, beta1:float = 0.5, beta2:float = 0.999):\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas = (beta1,beta2))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas = (beta1,beta2))\n",
    "    return g_optimizer, d_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_logits):\n",
    "    loss = -torch.mean(fake_logits)\n",
    "    return loss \n",
    "\n",
    "def discriminator_loss(real_logits, fake_logits):\n",
    "    loss = torch.mean(real_logits)-torch.mean(fake_logits)\n",
    "    return loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_penalty(discriminator, real_images, fake_images, device):\n",
    "    batch_size, c, h, w = real_images.shape\n",
    "    epsilon = torch.rand(batch_size, 1, 1, 1, device=device, requires_grad=True)\n",
    "    interpolated_images = epsilon * real_images + (1 - epsilon) * fake_images\n",
    "\n",
    "    mixed_scores = discriminator(interpolated_images)\n",
    "\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "\n",
    "    gradient = gradient.view(batch_size, -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_step(batch_size, latent_dim, real_images, generator, discriminator, d_optimizer, lambda_gp, device):\n",
    "    noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "    fake_images = generator(noise)\n",
    "    \n",
    "    real_logits = discriminator(real_images)\n",
    "    fake_logits = discriminator(fake_images.detach())\n",
    "    \n",
    "    gp = gradient_penalty(discriminator, real_images, fake_images, device)\n",
    "    d_loss = torch.mean(fake_logits) - torch.mean(real_logits) + lambda_gp * gp\n",
    "    \n",
    "    d_optimizer.zero_grad()\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    return {'loss': d_loss.item(), 'gp': gp.item()}\n",
    "\n",
    "def generator_step(batch_size, latent_dim, generator, discriminator, g_optimizer, device):\n",
    "    noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "    fake_images = generator(noise)\n",
    "    fake_logits = discriminator(fake_images)\n",
    "    \n",
    "    g_loss = -torch.mean(fake_logits)\n",
    "    \n",
    "    g_optimizer.zero_grad()\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    \n",
    "    return {'loss': g_loss.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "device = 'cuda'\n",
    "n_epochs = 4\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 50\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "g_optimizer, d_optimizer = create_optimizers(generator, discriminator)\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=64, \n",
    "                        shuffle=True, \n",
    "                        num_workers=4, \n",
    "                        drop_last=True,\n",
    "                        pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(fixed_latent_vector: torch.Tensor):\n",
    "    \"\"\" helper function to display images during training \"\"\"\n",
    "    fig = plt.figure(figsize=(14, 4))\n",
    "    plot_size = 16\n",
    "    for idx in np.arange(plot_size):\n",
    "        ax = fig.add_subplot(2, int(plot_size/2), idx+1, xticks=[], yticks=[])\n",
    "        img = fixed_latent_vector[idx, ...].detach().cpu().numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = denormalize(img)\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_latent_vector = torch.randn(16, latent_dim,1,1).float().cuda()\n",
    "losses = []\n",
    "lr = 0.0001\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "lambda_gp = 10  \n",
    "critic_steps = 5\n",
    "n_epochs = 100\n",
    "print_every = 10\n",
    "\n",
    "g_optimizer, d_optimizer = create_optimizers(generator, discriminator, lr, beta1, beta2)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for batch_i, real_images in enumerate(dataloader):\n",
    "        real_images = real_images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        d_step_res = discriminator_step(batch_size, latent_dim, real_images, generator, discriminator, d_optimizer, lambda_gp, device)\n",
    "        d_loss_value = d_step_res['loss']\n",
    "        gp_value = d_step_res['gp']\n",
    "\n",
    "        g_step_res = generator_step(batch_size, latent_dim, generator, discriminator, g_optimizer, device)\n",
    "        g_loss_value = g_step_res['loss']\n",
    "\n",
    "        if batch_i % print_every == 0:\n",
    "            d = d_loss_value\n",
    "            g = g_loss_value\n",
    "            losses.append((d,g))\n",
    "            time = str(datetime.now()).split('.')[0]\n",
    "            print(f'{time} | Epoch [{epoch+1}/{n_epochs}] | Batch {batch_i}/{len(dataloader)} | d_loss: {d:.4f} | g_loss: {g:.4f}')\n",
    "    \n",
    "    # display images during training\n",
    "    generator.eval()\n",
    "    generated_images = generator(fixed_latent_vector)\n",
    "    display(generated_images)\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator', alpha=0.5)\n",
    "plt.plot(losses.T[1], label='Generator', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
